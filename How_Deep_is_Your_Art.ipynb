{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aghazahedim/How-Deep-is-Your-Art/blob/main/How_Deep_is_Your_Art.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GA5Ye65fFJsS"
      },
      "source": [
        "# This script borrows heavily from Ben Trevett PyTorch tutorials\n",
        "# https://github.com/bentrevett/pytorch-image-classification/tree/master?tab=readme-ov-file\n",
        "\n",
        "# Importing dependencies\n",
        "import copy\n",
        "import random\n",
        "import time\n",
        "import os\n",
        "import shutil\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.models as models\n",
        "from sklearn import manifold\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.utils.data as data\n",
        "import torch.nn.functional as F\n",
        "from sklearn import decomposition\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "from torch.optim.lr_scheduler import _LRScheduler\n",
        "\n",
        "# Constants\n",
        "EPOCHS = 10\n",
        "BATCH_SIZE = 8\n",
        "START_LR = 1e-7\n",
        "END_LR = 10\n",
        "NUM_ITER = 100\n",
        "FOUND_LR = 5e-4\n",
        "VALID_RATIO = 0.9\n",
        "TRAIN_RATIO = 0.8\n",
        "ROOT = 'replace with your own parent directory'\n",
        "min_seed = 1\n",
        "max_seed = 10000\n",
        "seed_list = [seed for seed in range(min_seed, max_seed+1)]\n",
        "pretrained_size = 224\n",
        "pretrained_means = [0.485, 0.456, 0.406]\n",
        "pretrained_stds= [0.229, 0.224, 0.225]\n",
        "vgg11_config = [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M']\n",
        "\n",
        "\n",
        "# Global Variables\n",
        "results = []\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "# Creating paths for (1) Dataset, (2) Train set, (3) Test set, (4) Classes\n",
        "data_dir = ROOT\n",
        "images_dir = os.path.join(data_dir, 'Dataset')\n",
        "train_dir = os.path.join(data_dir, 'Train')\n",
        "test_dir = os.path.join(data_dir, 'Test')\n",
        "classes = os.listdir(images_dir)\n",
        "os.makedirs('replace wirh your own directoy'.format(j = TRAIN_RATIO, ROOT = ROOT), exist_ok = True)\n",
        "\n",
        "\n",
        "\n",
        "# Functions\n",
        "\n",
        "# In-trainig Accuracy\n",
        "def calculate_accuracy(y_pred, y):\n",
        "  top_pred = y_pred.argmax(1, keepdim = True)\n",
        "  correct = top_pred.eq(y.view_as(top_pred)).sum()\n",
        "  acc = correct.float() / y.shape[0]\n",
        "  return acc\n",
        "\n",
        "# Training Function\n",
        "def train(model, iterator, optimizer, criterion, device):\n",
        "  epoch_loss = 0\n",
        "  epoch_acc = 0\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  for (x, y) in iterator:\n",
        "\n",
        "      x = x.to(device)\n",
        "      y = y.to(device)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      y_pred, _ = model(x)\n",
        "\n",
        "      loss = criterion(y_pred, y)\n",
        "\n",
        "      acc = calculate_accuracy(y_pred, y)\n",
        "\n",
        "      loss.backward()\n",
        "\n",
        "      optimizer.step()\n",
        "\n",
        "      epoch_loss += loss.item()\n",
        "      epoch_acc += acc.item()\n",
        "\n",
        "  return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
        "\n",
        "# Evaluating Function\n",
        "def evaluate(model, iterator, criterion, device, seed):\n",
        "  epoch_loss = 0\n",
        "  epoch_acc = 0\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "\n",
        "      for (x, y) in iterator:\n",
        "\n",
        "          x = x.to(device)\n",
        "          y = y.to(device)\n",
        "\n",
        "          y_pred, _ = model(x)\n",
        "\n",
        "          loss = criterion(y_pred, y)\n",
        "\n",
        "          acc = calculate_accuracy(y_pred, y)\n",
        "\n",
        "          epoch_loss += loss.item()\n",
        "          epoch_acc += acc.item()\n",
        "\n",
        "  if seed:\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator), SEED\n",
        "  else:\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
        "\n",
        "# Calculating the Epoch Time\n",
        "def epoch_time(start_time, end_time):\n",
        "  elapsed_time = end_time - start_time\n",
        "  elapsed_mins = int(elapsed_time / 60)\n",
        "  elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "  return elapsed_mins, elapsed_secs\n",
        "\n",
        "# Getting the model predictions\n",
        "def get_predictions(model, iterator):\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  images = []\n",
        "  labels = []\n",
        "  probs = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "\n",
        "      for (x, y) in iterator:\n",
        "\n",
        "          x = x.to(device)\n",
        "\n",
        "          y_pred, _ = model(x)\n",
        "\n",
        "          y_prob = F.softmax(y_pred, dim = -1)\n",
        "          top_pred = y_prob.argmax(1, keepdim = True)\n",
        "\n",
        "          images.append(x.cpu())\n",
        "          labels.append(y.cpu())\n",
        "          probs.append(y_prob.cpu())\n",
        "\n",
        "  images = torch.cat(images, dim = 0)\n",
        "  labels = torch.cat(labels, dim = 0)\n",
        "  probs = torch.cat(probs, dim = 0)\n",
        "\n",
        "  return images, labels, probs\n",
        "\n",
        "# Defining the VGG Model Class and\n",
        "# overwriting the required definitions\n",
        "class VGG(nn.Module):\n",
        "  def __init__(self, features, output_dim):\n",
        "    super().__init__()\n",
        "\n",
        "    self.features = features\n",
        "\n",
        "    self.avgpool = nn.AdaptiveAvgPool2d(7)\n",
        "\n",
        "    self.classifier = nn.Sequential(\n",
        "        nn.Linear(512 * 7 * 7, 4096),\n",
        "        nn.ReLU(inplace = True),\n",
        "        nn.Dropout(0.5),\n",
        "        nn.Linear(4096, 4096),\n",
        "        nn.ReLU(inplace = True),\n",
        "        nn.Dropout(0.5),\n",
        "        nn.Linear(4096, output_dim),\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.features(x)\n",
        "    x = self.avgpool(x)\n",
        "    h = x.view(x.shape[0], -1)\n",
        "    x = self.classifier(h)\n",
        "    return x, h\n",
        "\n",
        "def get_vgg_layers(config, batch_norm):\n",
        "  layers = []\n",
        "  in_channels = 3\n",
        "\n",
        "  for c in config:\n",
        "    assert c == 'M' or isinstance(c, int)\n",
        "    if c == 'M':\n",
        "      layers += [nn.MaxPool2d(kernel_size = 2)]\n",
        "    else:\n",
        "      conv2d = nn.Conv2d(in_channels, c, kernel_size = 3, padding = 1)\n",
        "      if batch_norm:\n",
        "        layers += [conv2d, nn.BatchNorm2d(c), nn.ReLU(inplace = True)]\n",
        "      else:\n",
        "        layers += [conv2d, nn.ReLU(inplace = True)]\n",
        "      in_channels = c\n",
        "\n",
        "  return nn.Sequential(*layers)\n",
        "\n",
        "# Learning Rate Finder Function\n",
        "class LRFinder:\n",
        "  def __init__(self, model, optimizer, criterion, device):\n",
        "\n",
        "    self.optimizer = optimizer\n",
        "    self.model = model\n",
        "    self.criterion = criterion\n",
        "    self.device = device\n",
        "\n",
        "    torch.save(model.state_dict(), 'init_params.pt')\n",
        "\n",
        "  def range_test(self, iterator, end_lr = 10, num_iter = 100,\n",
        "                   smooth_f = 0.05, diverge_th = 5):\n",
        "\n",
        "    lrs = []\n",
        "    losses = []\n",
        "    best_loss = float('inf')\n",
        "\n",
        "    lr_scheduler = ExponentialLR(self.optimizer, end_lr, num_iter)\n",
        "\n",
        "    iterator = IteratorWrapper(iterator)\n",
        "\n",
        "    for iteration in range(num_iter):\n",
        "\n",
        "        loss = self._train_batch(iterator)\n",
        "\n",
        "        lrs.append(lr_scheduler.get_last_lr()[0])\n",
        "\n",
        "        #update lr\n",
        "        lr_scheduler.step()\n",
        "\n",
        "        if iteration > 0:\n",
        "            loss = smooth_f * loss + (1 - smooth_f) * losses[-1]\n",
        "\n",
        "        if loss < best_loss:\n",
        "            best_loss = loss\n",
        "\n",
        "        losses.append(loss)\n",
        "\n",
        "        if loss > diverge_th * best_loss:\n",
        "            print(\"Stopping early, the loss has diverged\")\n",
        "            break\n",
        "\n",
        "    #reset model to initial parameters\n",
        "    model.load_state_dict(torch.load('init_params.pt'))\n",
        "\n",
        "\n",
        "    return lrs, losses\n",
        "\n",
        "  def _train_batch(self, iterator):\n",
        "\n",
        "    self.model.train()\n",
        "\n",
        "    self.optimizer.zero_grad()\n",
        "\n",
        "    x, y = iterator.get_batch()\n",
        "\n",
        "    x = x.to(self.device)\n",
        "    y = y.to(self.device)\n",
        "\n",
        "    y_pred, _ = self.model(x)\n",
        "\n",
        "    loss = self.criterion(y_pred, y)\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    self.optimizer.step()\n",
        "\n",
        "    return loss.item()\n",
        "# Exponential Scheduler Function\n",
        "class ExponentialLR(_LRScheduler):\n",
        "  def __init__(self, optimizer, end_lr, num_iter, last_epoch=-1):\n",
        "    self.end_lr = end_lr\n",
        "    self.num_iter = num_iter\n",
        "    super(ExponentialLR, self).__init__(optimizer, last_epoch)\n",
        "\n",
        "  def get_lr(self):\n",
        "    curr_iter = self.last_epoch\n",
        "    r = curr_iter / self.num_iter\n",
        "    return [base_lr * (self.end_lr / base_lr) ** r for base_lr in self.base_lrs]\n",
        "# Iterator Wrapper Function\n",
        "class IteratorWrapper:\n",
        "  def __init__(self, iterator):\n",
        "    self.iterator = iterator\n",
        "    self._iterator = iter(iterator)\n",
        "\n",
        "  def __next__(self):\n",
        "    try:\n",
        "      inputs, labels = next(self._iterator)\n",
        "    except StopIteration:\n",
        "      self._iterator = iter(self.iterator)\n",
        "      inputs, labels, *_ = next(self._iterator)\n",
        "\n",
        "    return inputs, labels\n",
        "\n",
        "  def get_batch(self):\n",
        "      return next(self)\n",
        "# Plotting Function for Learning Rate Finder\n",
        "def plot_lr_finder(lrs, losses, skip_start = 5, skip_end = 5):\n",
        "  if skip_end == 0:\n",
        "    lrs = lrs[skip_start:]\n",
        "    losses = losses[skip_start:]\n",
        "  else:\n",
        "    lrs = lrs[skip_start:-skip_end]\n",
        "    losses = losses[skip_start:-skip_end]\n",
        "\n",
        "  fig = plt.figure(figsize = (16,8))\n",
        "  ax = fig.add_subplot(1,1,1)\n",
        "  ax.plot(lrs, losses)\n",
        "  ax.set_xscale('log')\n",
        "  ax.set_xlabel('Learning rate')\n",
        "  ax.set_ylabel('Loss')\n",
        "  ax.grid(True, 'both', 'x')\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Process Function\n",
        "# (1) Wrapping the model as a unique random process - VGG()\n",
        "# (2) Number of unique random processes to run - num\n",
        "def VGG(num):\n",
        "\n",
        "  # Feeding a unique random number from the list to the seed variable\n",
        "  SEED = seed_list.pop(random.randint(min_seed,len(seed_list)))\n",
        "  # Synchronizing all the random processing use the unique random number\n",
        "  random.seed(SEED)\n",
        "  np.random.seed(SEED)\n",
        "  torch.manual_seed(SEED)\n",
        "  torch.cuda.manual_seed(SEED)\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "\n",
        "  # Data transformation for both sets\n",
        "  # (1) Data Augmentation\n",
        "  #     (a)Resize\n",
        "  #     (b)Random Rotation\n",
        "  #     (c)Random Horizontal Flip\n",
        "  #     (d)Random Crop\n",
        "  # (2) Normalization\n",
        "    train_transforms = transforms.Compose([\n",
        "                            transforms.Resize(pretrained_size),\n",
        "                            transforms.RandomRotation(5),\n",
        "                            transforms.RandomHorizontalFlip(0.5),\n",
        "                            transforms.RandomCrop(pretrained_size, padding = 10),\n",
        "                            transforms.ToTensor(),\n",
        "                            transforms.Normalize(mean = pretrained_means,\n",
        "                                                  std = pretrained_stds)\n",
        "                        ])\n",
        "\n",
        "  test_transforms = transforms.Compose([\n",
        "                            transforms.Resize(pretrained_size),\n",
        "                            transforms.CenterCrop(pretrained_size),\n",
        "                            transforms.ToTensor(),\n",
        "                            transforms.Normalize(mean = pretrained_means,\n",
        "                                                  std = pretrained_stds)\n",
        "                        ])\n",
        "\n",
        "  # Loading the data\n",
        "  train_data = datasets.ImageFolder(root = train_dir,\n",
        "                                    transform = train_transforms)\n",
        "\n",
        "  test_data = datasets.ImageFolder(root = test_dir,\n",
        "                                  transform = test_transforms)\n",
        "\n",
        "  # Create the validatoin splits\n",
        "  n_train_examples = int(len(train_data) * VALID_RATIO)\n",
        "  n_valid_examples = len(train_data) - n_train_examples\n",
        "\n",
        "  train_data, valid_data = data.random_split(train_data,\n",
        "                                            [n_train_examples, n_valid_examples])\n",
        "\n",
        "  valid_data = copy.deepcopy(valid_data)\n",
        "  valid_data.dataset.transform = test_transforms\n",
        "\n",
        "  # Uncomment to Verify the three sets\n",
        "  #print(f'Number of training examples: {len(train_data)}')\n",
        "  #print(f'Number of validation examples: {len(valid_data)}')\n",
        "  #print(f'Number of testing examples: {len(test_data)}')\n",
        "\n",
        "  # Get the classes\n",
        "  classes = test_data.classes\n",
        "  # Get the iterators\n",
        "  train_iterator = data.DataLoader(train_data,\n",
        "                                  shuffle = True,\n",
        "                                  batch_size = BATCH_SIZE)\n",
        "\n",
        "  valid_iterator = data.DataLoader(valid_data,\n",
        "                                  batch_size = BATCH_SIZE)\n",
        "\n",
        "  test_iterator = data.DataLoader(test_data,\n",
        "                                  batch_size = BATCH_SIZE)\n",
        "\n",
        "  # Configure the layers for VGG-11\n",
        "  vgg11_layers = get_vgg_layers(vgg11_config, batch_norm = True)\n",
        "\n",
        "  # Set the output dimensions according to the dataset\n",
        "  OUTPUT_DIM = classes\n",
        "  # Build the Model\n",
        "  model = VGG(vgg11_layers, OUTPUT_DIM)\n",
        "  # Load the pre-trained weitghs\n",
        "  pretrained_model = models.vgg11_bn(pretrained = True)\n",
        "  # Remove the classification/output layer\n",
        "  pretrained_model.classifier[-1]\n",
        "  # Get the restof the netowkr as the features for Transfer Learning\n",
        "  IN_FEATURES = pretrained_model.classifier[-1].in_features\n",
        "  # Create a fully connected layer for the new output layer\n",
        "  final_fc = nn.Linear(IN_FEATURES, OUTPUT_DIM)\n",
        "  # Replace the new output layer\n",
        "  pretrained_model.classifier[-1] = final_fc\n",
        "  # Load the state dictionary to the model\n",
        "  model.load_state_dict(pretrained_model.state_dict())\n",
        "\n",
        "  # Uncomment to use Learning Rate Finder\n",
        "  #lr_finder = LRFinder(model, optimizer, criterion, device)\n",
        "  #lrs, losses = lr_finder.range_test(train_iterator, END_LR, NUM_ITER)\n",
        "  # Uncomment to inspect the Learning Rate V.S. Loss plot\n",
        "  #plot_lr_finder(lrs, losses)\n",
        "  # Uncomment to compare Adam default value\n",
        "  #optimizer = optim.Adam(model.parameters(), lr = START_LR)\n",
        "\n",
        "  # Discriminitve Fine-tuning\n",
        "  # (1) model features at one order of magnitude lower\n",
        "  # (2) classification layer at the founded learnig rate\n",
        "  params = [\n",
        "            {'params': model.features.parameters(), 'lr': FOUND_LR / 10},\n",
        "            {'params': model.classifier.parameters()}\n",
        "          ]\n",
        "  # Get Adam Optimizer wiht the found learning rate\n",
        "  optimizer = optim.Adam(params, lr = FOUND_LR)\n",
        "  # Use GPU if available\n",
        "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "  # Get Cross Entropy Loss Fucntion\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  # Load the model and criterion to the device\n",
        "  model = model.to(device)\n",
        "  criterion = criterion.to(device)\n",
        "\n",
        "  # Uncomment to Verify the model\n",
        "  #print(f'Model: {model}')\n",
        "  # Uncomment to  Verify the pre-trained model\n",
        "  #print(f'Pretrained Model:{pretrained_model}')\n",
        "  # Uncomment to Verify the classifier\n",
        "  #print(f'Pretrained Classifier:{pretrained_model.classifier}')\n",
        "\n",
        "\n",
        "# Training the Model for the number of epochs\n",
        "  for epoch in range(EPOCHS):\n",
        "\n",
        "      start_time = time.monotonic()\n",
        "\n",
        "      train_loss, train_acc = train(model, train_iterator, optimizer, criterion, device)\n",
        "      valid_loss, valid_acc = evaluate(model, valid_iterator, criterion, device, False)\n",
        "\n",
        "      if valid_loss < best_valid_loss:\n",
        "          best_valid_loss = valid_loss\n",
        "          torch.save(model.state_dict(), 'best_weights.pt')\n",
        "\n",
        "      end_time = time.monotonic()\n",
        "\n",
        "      epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "      print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "      print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "      print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
        "\n",
        "  # Load the best model's parameter for infernece in the test set\n",
        "  model.load_state_dict(torch.load('best_weights.pt'))\n",
        "  results.append(evaluate(model, test_iterator, criterion, device, True))\n",
        "  test_loss, test_acc = evaluate(model, test_iterator, criterion, device)\n",
        "  # Print the Test Loss and Accuracy\n",
        "  print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')\n",
        "\n",
        "  # Get the predictions\n",
        "  images, labels, probs = get_predictions(model, test_iterator)\n",
        "  pred_labels = torch.argmax(probs, 1)\n",
        "\n",
        "  # Calculate the Metrics\n",
        "  stacked = torch.stack((labels, pred_labels), dim=1)\n",
        "  labe=[]\n",
        "  pred=[]\n",
        "  for row in stacked:\n",
        "      labels_, pred_labels_ = row.numpy()\n",
        "      labe.append(labels_)\n",
        "      pred.append(pred_labels_)\n",
        "\n",
        "  # Get the Confusion Matrix\n",
        "  cmatrix=confusion_matrix(labe, pred)\n",
        "  FP = cmatrix.sum(axis=0) - np.diag(cmatrix)\n",
        "  FN = cmatrix.sum(axis=1) - np.diag(cmatrix)\n",
        "  TP = np.diag(cmatrix)\n",
        "  TN = cmatrix.sum() - (FP + FN + TP)\n",
        "\n",
        "  # Overall Acc\n",
        "  ovAcc = np.sum(TP) / (np.sum(FN) + np.sum(TP))\n",
        "\n",
        "  # Sensitivity, hit rate, recall, or true positive rate\n",
        "  TPR = np.round(TP/(TP+FN),2)\n",
        "\n",
        "  # Specificity or true negative rate\n",
        "  TNR = np.round(TN/(TN+FP),2)\n",
        "\n",
        "  # Precision or positive predictive value\n",
        "  PPV = np.round(TP/(TP+FP),2)\n",
        "\n",
        "  # Negative predictive value\n",
        "  NPV = np.round(TN/(TN+FN),2)\n",
        "\n",
        "  # Fall out or false positive rate\n",
        "  FPR = np.round(FP/(FP+TN),2)\n",
        "\n",
        "  # False negative rate\n",
        "  FNR = np.round(FN/(TP+FN),2)\n",
        "\n",
        "  # False discovery rate\n",
        "  FDR = np.round(FP/(TP+FP),2)\n",
        "\n",
        "  # Overall accuracy\n",
        "  ACC = np.round((TP+TN)/(TP+FP+FN+TN),2)\n",
        "\n",
        "  # Aggregate the Metrics\n",
        "  tablevalue = np.vstack(([a[:6] for a in classes],TPR,TNR,PPV,NPV,FPR,FNR,FDR,ACC))\n",
        "  all_metrics = pd.DataFrame(tablevalue, index=['class','TPR','TNR','PPV','NPV','FPR','FNR','FDR','ACC'])\n",
        "\n",
        "\n",
        "\n",
        "# Iteration loop for runnig VGG with a unique seed\n",
        "for i in range(10):\n",
        "  VGG(i)\n",
        "\n",
        "\n",
        "# Printing the Metrics and Train/Test Results\n",
        "print(f'All Metrics DataFrame: {all_metrics}')\n",
        "print(f'All Iteration results: {results}')\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}